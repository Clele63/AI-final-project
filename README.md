# Projet d'IA : Classification de D√©p√¥ts GitHub

Ce projet impl√©mente un pipeline complet pour la classification de d√©p√¥ts GitHub. Il utilise une approche en deux √©tapes : d'abord, un clustering non supervis√© pour **d√©couvrir** des cat√©gories pertinentes, puis une classification supervis√©e pour **entra√Æner** un mod√®le capable de pr√©dire ces cat√©gories.

L'ensemble du pipeline est con√ßu pour √™tre ex√©cut√© sur un cluster de calcul via **SLURM** et utilise **`uv`** pour une gestion d'environnement rapide et reproductible.

## üöÄ Architecture du Pipeline

Le projet est divis√© en deux jobs SLURM principaux, g√©r√©s par `run_pipeline.sh` :

1.  **√âtape 1 : Cr√©ation des Cat√©gories (`step1_create_categories.py`)**
    * **Entr√©e** : `github_data_with_readmes.csv`
    * **T√¢che** : Utilise `all-MiniLM-L6-v2` pour g√©n√©rer des embeddings et `MiniBatchKMeans` pour cr√©er 200 clusters.
    * **Sortie** : `github_categories_database.json` (la base de donn√©es des cat√©gories) et des graphiques d'analyse (`cluster_distribution.png`).

2.  **√âtape 2 : Entra√Ænement du Classifieur (`step2_train_classifier.py`)**
    * **Entr√©e** : `github_data_with_readmes.csv` + le JSON de l'√©tape 1.
    * **T√¢che** : "Fine-tune" un mod√®le `distilroberta-base` pour la classification de s√©quences sur le GPU.
    * **Sortie** : Le mod√®le entra√Æn√© (`distilroberta_github_classifier/`), un rapport de classification (`classification_report.txt`) et des graphiques de performance (`training_plots.png`, `confusion_matrix.png`).

---

## üìã Pr√©requis

Avant de lancer le pipeline, assurez-vous de :

1.  Avoir install√© `uv` (ex: `pip install --user uv`).
2.  Avoir `git-lfs` install√© pour r√©cup√©rer le jeu de donn√©es.

## ‚ö° Guide de Lancement Rapide

1.  **R√©cup√©rer les donn√©es (Git LFS)**
    Assurez-vous que le fichier CSV de donn√©es est bien t√©l√©charg√© (et n'est pas juste un pointeur Git LFS) :
    ```bash
    git lfs pull
    ```

2.  **Adapter les scripts SBATCH**
    V√©rifiez que les scripts `run_step1.sbatch` et `run_step2.sbatch` ciblent la bonne partition SLURM (`--partition=...`) et utilisent le bon chemin absolu vers `uv`.

3.  **Lancer le Pipeline**
    Rendez le script de lancement ex√©cutable et lancez-le :
    ```bash
    chmod +x run_pipeline.sh
    ./run_pipeline.sh
    ```

4.  **Suivre l'ex√©cution**
    Vous pouvez suivre la file d'attente avec `squeue -u $USER` et voir les sorties en direct avec :
    ```bash
    tail -f slurm_logs/step1_categories-*.out
    tail -f slurm_logs/step2_training-*.out
    ```

---

## üìö Rapport Complet

Pour une analyse d√©taill√©e de la m√©thodologie, des d√©fis rencontr√©s et des r√©sultats, consultez le rapport complet :

**[Lire le rapport complet](./docs/rapport.md)**